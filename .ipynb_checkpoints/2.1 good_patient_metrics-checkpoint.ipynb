{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87be62f-69f8-40ae-bada-296c0c4fcd9f",
   "metadata": {},
   "source": [
    "# Notebook to compute good patient metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0163a2d-895c-477b-939b-d307753a6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import patient_metrics as pm\n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import  butter, filtfilt, lfilter\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## FUNCTIONS ###\n",
    "\n",
    "# functions for visibility that are in the compute_loading_intensity module\n",
    "# pass in 3 sensors\n",
    "def vector_magnitude(vectors):\n",
    "    n = len(vectors[0])\n",
    "    assert all(len(v) == n for v in vectors), \"Vectors have different lengths\"\n",
    "    vm = np.sqrt(sum(v ** 2 for v in vectors))\n",
    "    return vm\n",
    "\n",
    "def build_filter(frequency, sample_rate, filter_type, filter_order):\n",
    "    nyq = 0.5 * sample_rate\n",
    "\n",
    "    if filter_type == \"bandpass\":\n",
    "        nyq_cutoff = (frequency[0] / nyq, frequency[1] / nyq)\n",
    "        b, a = butter(filter_order, (frequency[0], frequency[1]), btype=filter_type, analog=False, output='ba', fs=sample_rate)\n",
    "    elif filter_type == \"low\":\n",
    "        nyq_cutoff = frequency[1] / nyq\n",
    "        b, a = butter(filter_order, frequency[1], btype=filter_type, analog=False, output='ba', fs=sample_rate)\n",
    "    else:\n",
    "        nyq_cutoff = frequency / nyq\n",
    "\n",
    "    return b, a\n",
    "\n",
    "def filter_signal(b, a, signal, filter):\n",
    "    if(filter==\"lfilter\"):\n",
    "        return lfilter(b, a, signal)\n",
    "    elif(filter==\"filtfilt\"):\n",
    "        return filtfilt(b, a, signal)\n",
    "    elif(filter==\"sos\"):\n",
    "        return sosfiltfilt(sos, signal)\n",
    "    \n",
    "\n",
    "def compute_fft_mag(data):\n",
    "    fftpoints = int(math.pow(2, math.ceil(math.log2(len(data)))))\n",
    "    #print(fftpoints)\n",
    "    fft = np.fft.fft(data, n=fftpoints)\n",
    "    mag = np.abs(fft) / (fftpoints/2)\n",
    "    return mag.tolist()\n",
    "\n",
    "\n",
    "def compute_loading_intensity(fft_magnitudes, sampling_frequency, high_cut_off):\n",
    "    fftpoints = int(math.pow(2, math.ceil(math.log2(len(fft_magnitudes)))))\n",
    "    LI = 0\n",
    "    fs = sampling_frequency\n",
    "    fc = high_cut_off\n",
    "    kc = int((fftpoints/fs)* fc) + 1\n",
    "\n",
    "    magnitudes = fft_magnitudes\n",
    "\n",
    "    f = []\n",
    "    for i in range(0, int(fftpoints/2)+1):\n",
    "        f.append((fs*i)/fftpoints)\n",
    "\n",
    "    for k in range(0, kc):\n",
    "        LI = LI + (magnitudes[k] * f[k])\n",
    "\n",
    "    return LI\n",
    "\n",
    "\n",
    "# computes the loading intensity in chunks\n",
    "def compute_weight_bearing(accel_x, accel_y, accel_z, sampling_rate, time_window, lc_off, hc_off, filter_order, filter_type):\n",
    "    # build the filter\n",
    "    b,a = build_filter((lc_off, hc_off), sampling_rate, filter_type, filter_order)\n",
    "    \n",
    "    accel_x = accel_x.to_numpy()  / 9.80665\n",
    "    accel_y = accel_y.to_numpy()  / 9.80665\n",
    "    accel_z = accel_z.to_numpy()  / 9.80665\n",
    "    \n",
    "    window_samples = time_window * sampling_rate\n",
    "    \n",
    "    # chunk the data\n",
    "    a_x = [accel_x[i:i + window_samples] for i in range(0, len(accel_x), window_samples)]\n",
    "    a_y = [accel_y[i:i + window_samples] for i in range(0, len(accel_y), window_samples)]\n",
    "    a_z = [accel_z[i:i + window_samples] for i in range(0, len(accel_z), window_samples)]\n",
    "\n",
    "    # for each chunk\n",
    "    li = []\n",
    "    for idx, chunk in enumerate(a_x):\n",
    "        a_mag = vector_magnitude([chunk, a_y[idx], a_z[idx]])\n",
    "        filtered_mag = filter_signal(b,a, a_mag, \"filtfilt\")\n",
    "        fft_mag = compute_fft_mag(filtered_mag)\n",
    "        li_result = compute_loading_intensity(fft_mag, sampling_rate, hc_off)\n",
    "        li.append(li_result)\n",
    "        \n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f558e533-84f5-489a-9c9a-f5d7b5e36402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dataframe(path):\n",
    "    df = pd.read_csv(path)\n",
    "    acc_df = df[[\"Acc_X\", \"Acc_Y\", \"Acc_Z\"]]\n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78956987-3a7b-4ae9-8f89-239c637e4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS ###\n",
    "\n",
    "filter_inputs = {\n",
    "    \"sampling_rate\": 60,\n",
    "    \"low_cut_off\": 0.1,\n",
    "    \"high_cut_off\": 6,\n",
    "    \"filter_type\": \"bandpass\",\n",
    "    \"filter_order\": 5,\n",
    "    \"window\": 5,\n",
    "    \"g\": 9.80665,\n",
    "}\n",
    "\n",
    "def convert_df_to_array(df, g):\n",
    "    return [df[\"Acc_X\"].to_numpy() / g, \n",
    "            df[\"Acc_Y\"].to_numpy() / g, \n",
    "            df[\"Acc_Z\"].to_numpy() / g\n",
    "           ]\n",
    "\n",
    "# input in the form [[x], [y], [z]] \n",
    "# return [[x_windowed], [y_windowed], [z_windowed]]\n",
    "def create_windowed_data_old(data, window_samples):\n",
    "    x = [data[0][i:i + window_samples] for i in range(0, len(data[0]), window_samples)]\n",
    "    y = [data[1][i:i + window_samples] for i in range(0, len(data[1]), window_samples)]    \n",
    "    z = [data[2][i:i + window_samples] for i in range(0, len(data[2]), window_samples)]\n",
    "    return [x,y,z]\n",
    "\n",
    "def create_windowed_data(data, window_samples):\n",
    "    x = []\n",
    "    for i in range(0, len(data[0]), window_samples):\n",
    "        chunk = data[0][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        x.append(chunk)\n",
    "\n",
    "    y = []\n",
    "    for i in range(0, len(data[1]), window_samples):\n",
    "        chunk = data[1][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        y.append(chunk)\n",
    "\n",
    "    z = []\n",
    "    for i in range(0, len(data[2]), window_samples):\n",
    "        chunk = data[2][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        z.append(chunk)\n",
    "\n",
    "    return [x,y,z]\n",
    "\n",
    "def compute_li_for_chunk(x_chunk, y_chunk, z_chunk, filter_inputs):\n",
    "    a_mag = vector_magnitude([x_chunk, y_chunk, z_chunk])\n",
    "    filtered_mag = filter_signal(filter_inputs[\"b\"],filter_inputs[\"a\"], a_mag, \"filtfilt\")\n",
    "    fft_mag = compute_fft_mag(filtered_mag)\n",
    "    li_result = compute_loading_intensity(fft_mag, filter_inputs[\"sampling_rate\"], filter_inputs[\"high_cut_off\"])\n",
    "    return li_result\n",
    "\n",
    "def compute_axis_li_for_chunk(axis_chunk, filter_inputs):\n",
    "    filtered_axis = filter_signal(filter_inputs[\"b\"],filter_inputs[\"a\"], axis_chunk, \"filtfilt\")\n",
    "    fft_axis = compute_fft_mag(filtered_axis)\n",
    "    li_result = compute_loading_intensity(fft_axis, filter_inputs[\"sampling_rate\"], filter_inputs[\"high_cut_off\"])\n",
    "    return li_result\n",
    "    \n",
    "\n",
    "def compute_loading_metrics_from_assessment(left_df, right_df, filter_inputs, p_num):\n",
    "    # 0. \n",
    "    b,a = build_filter((filter_inputs[\"low_cut_off\"], filter_inputs[\"high_cut_off\"]), \n",
    "                       filter_inputs[\"sampling_rate\"], \n",
    "                       filter_inputs[\"filter_type\"], \n",
    "                       filter_inputs[\"filter_order\"])\n",
    "    filter_inputs[\"b\"] = b\n",
    "    filter_inputs[\"a\"] = a\n",
    "    \n",
    "    window_samples = filter_inputs[\"window\"] * filter_inputs[\"sampling_rate\"]\n",
    "    \n",
    "    # 1. convert the df to numpy arrays for each axis\n",
    "    # returns [[x], [y], [z]] \n",
    "    l_acc = convert_df_to_array(left_df, filter_inputs[\"g\"])\n",
    "    r_acc = convert_df_to_array(right_df, filter_inputs[\"g\"])\n",
    "\n",
    "    # 2. chunk the data into windows \n",
    "    l_windows = create_windowed_data(l_acc, window_samples)\n",
    "    r_windows = create_windowed_data(r_acc, window_samples)\n",
    "\n",
    "    # assume that the number of windows in l and r are the same\n",
    "    #if (len(l_windows[0]) == len(r_windows[0])):\n",
    "    #    print(\"window lengths are equal\")\n",
    "    #else:\n",
    "    #    print(\"window lengths are NOT equal\")\n",
    "       \n",
    "    # 3. compute loading intensity\n",
    "    l_li_result = []\n",
    "    l_li_x_result = []\n",
    "    l_li_y_result = []\n",
    "    l_li_z_result = []\n",
    "    r_li_result = []\n",
    "    r_li_x_result = []\n",
    "    r_li_y_result = []\n",
    "    r_li_z_result = []\n",
    "    \n",
    "    # change this for loop to be cleaner\n",
    "    for idx, chunk in enumerate(l_windows[0]):\n",
    "        l_li = compute_li_for_chunk(l_windows[0][idx], l_windows[1][idx], l_windows[2][idx], filter_inputs)\n",
    "        l_li_result.append(l_li)\n",
    "        r_li = compute_li_for_chunk(r_windows[0][idx], r_windows[1][idx], r_windows[2][idx], filter_inputs)\n",
    "        r_li_result.append(r_li)\n",
    "\n",
    "        # pass the axis only\n",
    "        l_li_x = compute_axis_li_for_chunk(l_windows[0][idx], filter_inputs)\n",
    "        l_li_y = compute_axis_li_for_chunk(l_windows[1][idx], filter_inputs)\n",
    "        l_li_z = compute_axis_li_for_chunk(l_windows[2][idx], filter_inputs)\n",
    "\n",
    "        l_li_x_result.append(l_li_x)\n",
    "        l_li_y_result.append(l_li_y)\n",
    "        l_li_z_result.append(l_li_z)\n",
    "\n",
    "        r_li_x = compute_axis_li_for_chunk(r_windows[0][idx], filter_inputs)\n",
    "        r_li_y = compute_axis_li_for_chunk(r_windows[1][idx], filter_inputs)\n",
    "        r_li_z = compute_axis_li_for_chunk(r_windows[2][idx], filter_inputs)\n",
    "\n",
    "        r_li_x_result.append(r_li_x)\n",
    "        r_li_y_result.append(r_li_y)\n",
    "        r_li_z_result.append(r_li_z)\n",
    "    \n",
    "    # 4. compute the metrics\n",
    "\n",
    "    # capability\n",
    "    l_avg_load = round(np.sum(l_li_result) / len(l_li_result),2)\n",
    "    r_avg_load = round(np.sum(r_li_result) / len(r_li_result),2)\n",
    "    overall_load = round((l_avg_load + r_avg_load) / 2, 2)\n",
    "\n",
    "    # distribution\n",
    "    load_in_left = round((l_avg_load / (l_avg_load + r_avg_load)) * 100, 2)\n",
    "    load_in_right = round((r_avg_load / (l_avg_load + r_avg_load)) * 100, 2)\n",
    "    imbalance = round(abs(load_in_left - load_in_right),2)\n",
    "\n",
    "    # make a function?\n",
    "    l_t_axis = np.mean([sum(elements) for elements in zip(l_li_x_result, l_li_y_result, l_li_z_result)])\n",
    "    r_t_axis = np.mean([sum(elements) for elements in zip(r_li_x_result, r_li_y_result, r_li_z_result)])\n",
    "    \n",
    "    left_avg_axis = []\n",
    "    right_avg_axis = []\n",
    "    left_percent_contribution = []\n",
    "    right_percent_contribution = []\n",
    "    for i in range(0,3): # for each axis x, y, z\n",
    "        #left\n",
    "        l_x_avg = round(np.sum(l_li_x_result)/len(l_li_x_result),2)\n",
    "        l_y_avg = round(np.sum(l_li_y_result)/len(l_li_y_result),2)\n",
    "        l_z_avg = round(np.sum(l_li_z_result)/len(l_li_z_result),2)\n",
    "        left_avg_axis.append(l_x_avg)\n",
    "        left_avg_axis.append(l_y_avg)\n",
    "        left_avg_axis.append(l_z_avg)\n",
    "        \n",
    "        left_percent_contribution.append(round((l_x_avg/l_t_axis)*100,0))\n",
    "        left_percent_contribution.append(round((l_y_avg/l_t_axis)*100,0))\n",
    "        left_percent_contribution.append(round((l_z_avg/l_t_axis)*100,0))\n",
    "\n",
    "        #right\n",
    "        r_x_avg = round(np.sum(r_li_x_result)/len(r_li_x_result),2)\n",
    "        r_y_avg = round(np.sum(r_li_y_result)/len(r_li_y_result),2)\n",
    "        r_z_avg = round(np.sum(r_li_z_result)/len(r_li_z_result),2)\n",
    "        right_avg_axis.append(r_x_avg)\n",
    "        right_avg_axis.append(r_y_avg)\n",
    "        right_avg_axis.append(r_z_avg)\n",
    "        \n",
    "        right_percent_contribution.append(round((r_x_avg/r_t_axis)*100,0))\n",
    "        right_percent_contribution.append(round((r_y_avg/r_t_axis)*100,0))\n",
    "        right_percent_contribution.append(round((r_z_avg/r_t_axis)*100,0))\n",
    "\n",
    "    # axial contribution\n",
    "    r_v_imbalance = round((right_avg_axis[0] / (left_avg_axis[0] + right_avg_axis[0]))*100,2)\n",
    "    l_v_imbalance = round((left_avg_axis[0] / (left_avg_axis[0] + right_avg_axis[0]))*100,2)\n",
    "    v_imbalance = round(abs(l_v_imbalance-r_v_imbalance),2)\n",
    "    r_ml_imbalance = round((right_avg_axis[1] / (left_avg_axis[1] + right_avg_axis[1]))*100,2)\n",
    "    l_ml_imbalance = round((left_avg_axis[1] / (left_avg_axis[1] + right_avg_axis[1]))*100,2)\n",
    "    ml_imbalance = round(abs(l_ml_imbalance-r_ml_imbalance),2)\n",
    "    r_ap_imbalance = round((right_avg_axis[2] / (left_avg_axis[2] + right_avg_axis[2]))*100,2)\n",
    "    l_ap_imbalance = round((left_avg_axis[2] / (left_avg_axis[2] + right_avg_axis[2]))*100,2)\n",
    "    ap_imbalance = round(abs(l_ap_imbalance-r_ap_imbalance),2)\n",
    "    p_metrics = {\n",
    "        \"patient_number\": p_num,\n",
    "        \"left_avg_load\": l_avg_load,\n",
    "        \"right_avg_load\": r_avg_load,\n",
    "        \"overall_load\" : overall_load,\n",
    "        \"load_in_left\": load_in_left,\n",
    "        \"load_in_right\": load_in_right,\n",
    "        \"imbalance\": imbalance,\n",
    "        \"left_x_avg_load\": left_avg_axis[0],\n",
    "        \"left_y_avg_load\": left_avg_axis[1],\n",
    "        \"left_z_avg_load\": left_avg_axis[2],\n",
    "        \"left_x_contribution\": left_percent_contribution[0],\n",
    "        \"left_y_contribution\": left_percent_contribution[1],\n",
    "        \"left_z_contribution\": left_percent_contribution[2],\n",
    "        \"right_x_avg_load\": right_avg_axis[0],\n",
    "        \"right_y_avg_load\": right_avg_axis[1],\n",
    "        \"right_z_avg_load\": right_avg_axis[2],\n",
    "        \"right_x_contribution\": right_percent_contribution[0],\n",
    "        \"right_y_contribution\": right_percent_contribution[1],\n",
    "        \"right_z_contribution\": right_percent_contribution[2],\n",
    "        \"v_imbalance\": v_imbalance,\n",
    "        \"ml_imbalance\": ml_imbalance,\n",
    "        \"ap_imbalance\": ap_imbalance,\n",
    "    }\n",
    "    return p_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bf91f0c-ce03-4594-a1b9-e9c62f925bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "patient_data_path = \"Test_data/good/\"\n",
    "# List of directories to exclude\n",
    "excluded_dirs = {'.ipynb_checkpoints'}  # Use a set for faster lookups\n",
    "\n",
    "# List subdirectories, excluding the specified directories\n",
    "subdirs = [d for d in os.listdir(patient_data_path)\n",
    "           if os.path.isdir(os.path.join(patient_data_path, d)) and d not in excluded_dirs]\n",
    "print(subdirs)\n",
    "\n",
    "patient_data_files = []\n",
    "# Loop through each subdirectory\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(patient_data_path, subdir)    \n",
    "    # Use glob to find all files in the current subdirectory\n",
    "    files = glob.glob(os.path.join(subdir_path, '*.csv'))  # Get all files\n",
    "    if(subdir==str(6)):\n",
    "        print(files[len(files)-2 : len(files)])  # this is correct for patient 6\n",
    "    patient_data_files.append((subdir, files[len(files)-2 : len(files)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e960530-a535-463c-b5a3-a2e13e43ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute metrics for patient: 1\n",
      "compute metrics for patient: 2\n",
      "compute metrics for patient: 3\n"
     ]
    }
   ],
   "source": [
    "patient_metrics = []\n",
    "for p_num, files in patient_data_files:\n",
    "    print(f\"compute metrics for patient: {p_num}\")\n",
    "    left_data = read_csv_to_dataframe(files[0])\n",
    "    right_data = read_csv_to_dataframe(files[1])\n",
    "    p_metrics = compute_loading_metrics_from_assessment(\n",
    "        left_data, \n",
    "        right_data, \n",
    "        filter_inputs,\n",
    "        p_num)\n",
    "    patient_metrics.append(p_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "323f37d7-1af2-4f62-a140-abb1a0678686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_patient_data = sorted(patient_metrics, key=lambda item: int(item['patient_number']), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61d61463-cbca-42e7-ae21-82766da7b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "p_df = pd.DataFrame(sorted_patient_data)\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = \"good_patient_metrics.csv\"\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "p_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb3f76-b407-4c8e-a85d-10610c1ca685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
