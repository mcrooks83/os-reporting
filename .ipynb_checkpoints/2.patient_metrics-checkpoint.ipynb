{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87be62f-69f8-40ae-bada-296c0c4fcd9f",
   "metadata": {},
   "source": [
    "### Notebook to compute metrics\n",
    "\n",
    "for each patient compute the following example:\n",
    "\n",
    "1. average load left \n",
    "2. average load right \n",
    "3. overall capability \n",
    "4. distribution - Left  Right\n",
    "5. imbalance\n",
    "6. left:  V  ML  AP \n",
    "7. left contribution V  ML  AP\n",
    "8. right:  V  ML AP \n",
    "9. right contribution V  ML  AP \n",
    "10. l axis contribution: v, ml, ap \n",
    "11. r axis contribution: v, ml, ap\n",
    "12. axis imbalance:      v, ml, ap \n",
    "\n",
    "#### output should be a datframe that can be saved as a csv\n",
    "#### each patient is a row and each metric above is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0163a2d-895c-477b-939b-d307753a6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import patient_metrics as pm\n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import  butter, filtfilt, lfilter\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## FUNCTIONS ###\n",
    "\n",
    "# functions for visibility that are in the compute_loading_intensity module\n",
    "# pass in 3 sensors\n",
    "def vector_magnitude(vectors):\n",
    "    n = len(vectors[0])\n",
    "    assert all(len(v) == n for v in vectors), \"Vectors have different lengths\"\n",
    "    vm = np.sqrt(sum(v ** 2 for v in vectors))\n",
    "    return vm\n",
    "\n",
    "def build_filter(frequency, sample_rate, filter_type, filter_order):\n",
    "    nyq = 0.5 * sample_rate\n",
    "\n",
    "    if filter_type == \"bandpass\":\n",
    "        nyq_cutoff = (frequency[0] / nyq, frequency[1] / nyq)\n",
    "        b, a = butter(filter_order, (frequency[0], frequency[1]), btype=filter_type, analog=False, output='ba', fs=sample_rate)\n",
    "    elif filter_type == \"low\":\n",
    "        nyq_cutoff = frequency[1] / nyq\n",
    "        b, a = butter(filter_order, frequency[1], btype=filter_type, analog=False, output='ba', fs=sample_rate)\n",
    "    else:\n",
    "        nyq_cutoff = frequency / nyq\n",
    "\n",
    "    return b, a\n",
    "\n",
    "def filter_signal(b, a, signal, filter):\n",
    "    if(filter==\"lfilter\"):\n",
    "        return lfilter(b, a, signal)\n",
    "    elif(filter==\"filtfilt\"):\n",
    "        return filtfilt(b, a, signal)\n",
    "    elif(filter==\"sos\"):\n",
    "        return sosfiltfilt(sos, signal)\n",
    "    \n",
    "\n",
    "def compute_fft_mag(data):\n",
    "    fftpoints = int(math.pow(2, math.ceil(math.log2(len(data)))))\n",
    "    #print(fftpoints)\n",
    "    fft = np.fft.fft(data, n=fftpoints)\n",
    "    mag = np.abs(fft) / (fftpoints/2)\n",
    "    return mag.tolist()\n",
    "\n",
    "\n",
    "def compute_loading_intensity(fft_magnitudes, sampling_frequency, high_cut_off):\n",
    "    fftpoints = int(math.pow(2, math.ceil(math.log2(len(fft_magnitudes)))))\n",
    "    LI = 0\n",
    "    fs = sampling_frequency\n",
    "    fc = high_cut_off\n",
    "    kc = int((fftpoints/fs)* fc) + 1\n",
    "\n",
    "    magnitudes = fft_magnitudes\n",
    "\n",
    "    f = []\n",
    "    for i in range(0, int(fftpoints/2)+1):\n",
    "        f.append((fs*i)/fftpoints)\n",
    "\n",
    "    for k in range(0, kc):\n",
    "        LI = LI + (magnitudes[k] * f[k])\n",
    "\n",
    "    return LI\n",
    "\n",
    "\n",
    "# computes the loading intensity in chunks\n",
    "def compute_weight_bearing(accel_x, accel_y, accel_z, sampling_rate, time_window, lc_off, hc_off, filter_order, filter_type):\n",
    "    # build the filter\n",
    "    b,a = build_filter((lc_off, hc_off), sampling_rate, filter_type, filter_order)\n",
    "    \n",
    "    accel_x = accel_x.to_numpy()  / 9.80665\n",
    "    accel_y = accel_y.to_numpy()  / 9.80665\n",
    "    accel_z = accel_z.to_numpy()  / 9.80665\n",
    "    \n",
    "    window_samples = time_window * sampling_rate\n",
    "    \n",
    "    # chunk the data\n",
    "    a_x = [accel_x[i:i + window_samples] for i in range(0, len(accel_x), window_samples)]\n",
    "    a_y = [accel_y[i:i + window_samples] for i in range(0, len(accel_y), window_samples)]\n",
    "    a_z = [accel_z[i:i + window_samples] for i in range(0, len(accel_z), window_samples)]\n",
    "\n",
    "    # for each chunk\n",
    "    li = []\n",
    "    for idx, chunk in enumerate(a_x):\n",
    "        a_mag = vector_magnitude([chunk, a_y[idx], a_z[idx]])\n",
    "        filtered_mag = filter_signal(b,a, a_mag, \"filtfilt\")\n",
    "        fft_mag = compute_fft_mag(filtered_mag)\n",
    "        li_result = compute_loading_intensity(fft_mag, sampling_rate, hc_off)\n",
    "        li.append(li_result)\n",
    "        \n",
    "    return li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f558e533-84f5-489a-9c9a-f5d7b5e36402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dataframe(path):\n",
    "    df = pd.read_csv(path)\n",
    "    acc_df = df[[\"Acc_X\", \"Acc_Y\", \"Acc_Z\"]]\n",
    "    return acc_df\n",
    "\n",
    "## TEST DATA ###\n",
    "p=2\n",
    "left_path = \"Test_data/2_1717405593622_walk_Left.csv\"\n",
    "right_path = \"Test_data/2_1717405593622_walk_Right.csv\"\n",
    "\n",
    "#p=13\n",
    "#left_path = \"patient_data/13/13_1718622130936_walk1_Left.csv\"\n",
    "#right_path = \"patient_data/13/13_1718622130936_walk1_Right.csv\"\n",
    "#p=17\n",
    "\n",
    "left_data = read_csv_to_dataframe(left_path)\n",
    "right_data = read_csv_to_dataframe(right_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78956987-3a7b-4ae9-8f89-239c637e4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS ###\n",
    "\n",
    "filter_inputs = {\n",
    "    \"sampling_rate\": 60,\n",
    "    \"low_cut_off\": 0.1,\n",
    "    \"high_cut_off\": 6,\n",
    "    \"filter_type\": \"bandpass\",\n",
    "    \"filter_order\": 5,\n",
    "    \"window\": 5,\n",
    "    \"g\": 9.80665,\n",
    "}\n",
    "\n",
    "def convert_df_to_array(df, g):\n",
    "    return [df[\"Acc_X\"].to_numpy() / g, \n",
    "            df[\"Acc_Y\"].to_numpy() / g, \n",
    "            df[\"Acc_Z\"].to_numpy() / g\n",
    "           ]\n",
    "\n",
    "# input in the form [[x], [y], [z]] \n",
    "# return [[x_windowed], [y_windowed], [z_windowed]]\n",
    "def create_windowed_data_old(data, window_samples):\n",
    "    x = [data[0][i:i + window_samples] for i in range(0, len(data[0]), window_samples)]\n",
    "    y = [data[1][i:i + window_samples] for i in range(0, len(data[1]), window_samples)]    \n",
    "    z = [data[2][i:i + window_samples] for i in range(0, len(data[2]), window_samples)]\n",
    "    return [x,y,z]\n",
    "\n",
    "def create_windowed_data(data, window_samples):\n",
    "    x = []\n",
    "    for i in range(0, len(data[0]), window_samples):\n",
    "        chunk = data[0][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        x.append(chunk)\n",
    "\n",
    "    y = []\n",
    "    for i in range(0, len(data[1]), window_samples):\n",
    "        chunk = data[1][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        y.append(chunk)\n",
    "\n",
    "    z = []\n",
    "    for i in range(0, len(data[2]), window_samples):\n",
    "        chunk = data[2][i:i + window_samples]  # Get the current chunk\n",
    "        if len(chunk) < window_samples:\n",
    "            pad = window_samples - len(chunk)\n",
    "            zeros = [0] * pad\n",
    "            chunk = np.concatenate((chunk, zeros))  # Concatenate the chunk and zeros\n",
    "        z.append(chunk)\n",
    "\n",
    "    return [x,y,z]\n",
    "\n",
    "def compute_li_for_chunk(x_chunk, y_chunk, z_chunk, filter_inputs):\n",
    "    a_mag = vector_magnitude([x_chunk, y_chunk, z_chunk])\n",
    "    filtered_mag = filter_signal(filter_inputs[\"b\"],filter_inputs[\"a\"], a_mag, \"filtfilt\")\n",
    "    fft_mag = compute_fft_mag(filtered_mag)\n",
    "    li_result = compute_loading_intensity(fft_mag, filter_inputs[\"sampling_rate\"], filter_inputs[\"high_cut_off\"])\n",
    "    return li_result\n",
    "\n",
    "def compute_axis_li_for_chunk(axis_chunk, filter_inputs):\n",
    "    filtered_axis = filter_signal(filter_inputs[\"b\"],filter_inputs[\"a\"], axis_chunk, \"filtfilt\")\n",
    "    fft_axis = compute_fft_mag(filtered_axis)\n",
    "    li_result = compute_loading_intensity(fft_axis, filter_inputs[\"sampling_rate\"], filter_inputs[\"high_cut_off\"])\n",
    "    return li_result\n",
    "    \n",
    "\n",
    "def compute_loading_metrics_from_assessment(left_df, right_df, filter_inputs, p_num):\n",
    "    # 0. \n",
    "    b,a = build_filter((filter_inputs[\"low_cut_off\"], filter_inputs[\"high_cut_off\"]), \n",
    "                       filter_inputs[\"sampling_rate\"], \n",
    "                       filter_inputs[\"filter_type\"], \n",
    "                       filter_inputs[\"filter_order\"])\n",
    "    filter_inputs[\"b\"] = b\n",
    "    filter_inputs[\"a\"] = a\n",
    "    \n",
    "    window_samples = filter_inputs[\"window\"] * filter_inputs[\"sampling_rate\"]\n",
    "    \n",
    "    # 1. convert the df to numpy arrays for each axis\n",
    "    # returns [[x], [y], [z]] \n",
    "    l_acc = convert_df_to_array(left_df, filter_inputs[\"g\"])\n",
    "    r_acc = convert_df_to_array(right_df, filter_inputs[\"g\"])\n",
    "\n",
    "    # 2. chunk the data into windows \n",
    "    l_windows = create_windowed_data(l_acc, window_samples)\n",
    "    r_windows = create_windowed_data(r_acc, window_samples)\n",
    "\n",
    "    # assume that the number of windows in l and r are the same\n",
    "    #if (len(l_windows[0]) == len(r_windows[0])):\n",
    "    #    print(\"window lengths are equal\")\n",
    "    #else:\n",
    "    #    print(\"window lengths are NOT equal\")\n",
    "       \n",
    "    # 3. compute loading intensity\n",
    "    l_li_result = []\n",
    "    l_li_x_result = []\n",
    "    l_li_y_result = []\n",
    "    l_li_z_result = []\n",
    "    r_li_result = []\n",
    "    r_li_x_result = []\n",
    "    r_li_y_result = []\n",
    "    r_li_z_result = []\n",
    "    \n",
    "    # change this for loop to be cleaner\n",
    "    for idx, chunk in enumerate(l_windows[0]):\n",
    "        l_li = compute_li_for_chunk(l_windows[0][idx], l_windows[1][idx], l_windows[2][idx], filter_inputs)\n",
    "        l_li_result.append(l_li)\n",
    "        r_li = compute_li_for_chunk(r_windows[0][idx], r_windows[1][idx], r_windows[2][idx], filter_inputs)\n",
    "        r_li_result.append(r_li)\n",
    "\n",
    "        # pass the axis only\n",
    "        l_li_x = compute_axis_li_for_chunk(l_windows[0][idx], filter_inputs)\n",
    "        l_li_y = compute_axis_li_for_chunk(l_windows[1][idx], filter_inputs)\n",
    "        l_li_z = compute_axis_li_for_chunk(l_windows[2][idx], filter_inputs)\n",
    "\n",
    "        l_li_x_result.append(l_li_x)\n",
    "        l_li_y_result.append(l_li_y)\n",
    "        l_li_z_result.append(l_li_z)\n",
    "\n",
    "        r_li_x = compute_axis_li_for_chunk(r_windows[0][idx], filter_inputs)\n",
    "        r_li_y = compute_axis_li_for_chunk(r_windows[1][idx], filter_inputs)\n",
    "        r_li_z = compute_axis_li_for_chunk(r_windows[2][idx], filter_inputs)\n",
    "\n",
    "        r_li_x_result.append(r_li_x)\n",
    "        r_li_y_result.append(r_li_y)\n",
    "        r_li_z_result.append(r_li_z)\n",
    "    \n",
    "    # 4. compute the metrics\n",
    "\n",
    "    # capability\n",
    "    l_avg_load = round(np.sum(l_li_result) / len(l_li_result),2)\n",
    "    r_avg_load = round(np.sum(r_li_result) / len(r_li_result),2)\n",
    "    overall_load = round((l_avg_load + r_avg_load) / 2, 2)\n",
    "\n",
    "    # distribution\n",
    "    load_in_left = round((l_avg_load / (l_avg_load + r_avg_load)) * 100, 2)\n",
    "    load_in_right = round((r_avg_load / (l_avg_load + r_avg_load)) * 100, 2)\n",
    "    imbalance = round(abs(load_in_left - load_in_right),2)\n",
    "\n",
    "    # make a function?\n",
    "    l_t_axis = np.mean([sum(elements) for elements in zip(l_li_x_result, l_li_y_result, l_li_z_result)])\n",
    "    r_t_axis = np.mean([sum(elements) for elements in zip(r_li_x_result, r_li_y_result, r_li_z_result)])\n",
    "    \n",
    "    left_avg_axis = []\n",
    "    right_avg_axis = []\n",
    "    left_percent_contribution = []\n",
    "    right_percent_contribution = []\n",
    "    for i in range(0,3): # for each axis x, y, z\n",
    "        #left\n",
    "        l_x_avg = round(np.sum(l_li_x_result)/len(l_li_x_result),2)\n",
    "        l_y_avg = round(np.sum(l_li_y_result)/len(l_li_y_result),2)\n",
    "        l_z_avg = round(np.sum(l_li_z_result)/len(l_li_z_result),2)\n",
    "        left_avg_axis.append(l_x_avg)\n",
    "        left_avg_axis.append(l_y_avg)\n",
    "        left_avg_axis.append(l_z_avg)\n",
    "        \n",
    "        left_percent_contribution.append(round((l_x_avg/l_t_axis)*100,0))\n",
    "        left_percent_contribution.append(round((l_y_avg/l_t_axis)*100,0))\n",
    "        left_percent_contribution.append(round((l_z_avg/l_t_axis)*100,0))\n",
    "\n",
    "        #right\n",
    "        r_x_avg = round(np.sum(r_li_x_result)/len(r_li_x_result),2)\n",
    "        r_y_avg = round(np.sum(r_li_y_result)/len(r_li_y_result),2)\n",
    "        r_z_avg = round(np.sum(r_li_z_result)/len(r_li_z_result),2)\n",
    "        right_avg_axis.append(r_x_avg)\n",
    "        right_avg_axis.append(r_y_avg)\n",
    "        right_avg_axis.append(r_z_avg)\n",
    "        \n",
    "        right_percent_contribution.append(round((r_x_avg/r_t_axis)*100,0))\n",
    "        right_percent_contribution.append(round((r_y_avg/r_t_axis)*100,0))\n",
    "        right_percent_contribution.append(round((r_z_avg/r_t_axis)*100,0))\n",
    "\n",
    "    # axial contribution\n",
    "    r_v_imbalance = round((right_avg_axis[0] / (left_avg_axis[0] + right_avg_axis[0]))*100,2)\n",
    "    l_v_imbalance = round((left_avg_axis[0] / (left_avg_axis[0] + right_avg_axis[0]))*100,2)\n",
    "    v_imbalance = round(abs(l_v_imbalance-r_v_imbalance),2)\n",
    "    r_ml_imbalance = round((right_avg_axis[1] / (left_avg_axis[1] + right_avg_axis[1]))*100,2)\n",
    "    l_ml_imbalance = round((left_avg_axis[1] / (left_avg_axis[1] + right_avg_axis[1]))*100,2)\n",
    "    ml_imbalance = round(abs(l_ml_imbalance-r_ml_imbalance),2)\n",
    "    r_ap_imbalance = round((right_avg_axis[2] / (left_avg_axis[2] + right_avg_axis[2]))*100,2)\n",
    "    l_ap_imbalance = round((left_avg_axis[2] / (left_avg_axis[2] + right_avg_axis[2]))*100,2)\n",
    "    ap_imbalance = round(abs(l_ap_imbalance-r_ap_imbalance),2)\n",
    "    p_metrics = {\n",
    "        \"patient_number\": p_num,\n",
    "        \"left_avg_load\": l_avg_load,\n",
    "        \"right_avg_load\": r_avg_load,\n",
    "        \"overall_load\" : overall_load,\n",
    "        \"load_in_left\": load_in_left,\n",
    "        \"load_in_right\": load_in_right,\n",
    "        \"imbalance\": imbalance,\n",
    "        \"left_x_avg_load\": left_avg_axis[0],\n",
    "        \"left_y_avg_load\": left_avg_axis[1],\n",
    "        \"left_z_avg_load\": left_avg_axis[2],\n",
    "        \"left_x_contribution\": left_percent_contribution[0],\n",
    "        \"left_y_contribution\": left_percent_contribution[1],\n",
    "        \"left_z_contribution\": left_percent_contribution[2],\n",
    "        \"right_x_avg_load\": right_avg_axis[0],\n",
    "        \"right_y_avg_load\": right_avg_axis[1],\n",
    "        \"right_z_avg_load\": right_avg_axis[2],\n",
    "        \"right_x_contribution\": right_percent_contribution[0],\n",
    "        \"right_y_contribution\": right_percent_contribution[1],\n",
    "        \"right_z_contribution\": right_percent_contribution[2],\n",
    "        \"v_imbalance\": v_imbalance,\n",
    "        \"ml_imbalance\": ml_imbalance,\n",
    "        \"ap_imbalance\": ap_imbalance,\n",
    "    }\n",
    "    return p_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59e9de13-3c6e-4f26-b32d-703a37e7cedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"patient_number\": 2,\n",
      "  \"left_avg_load\": 2.71,\n",
      "  \"right_avg_load\": 2.88,\n",
      "  \"overall_load\": 2.8,\n",
      "  \"load_in_left\": 48.48,\n",
      "  \"load_in_right\": 51.52,\n",
      "  \"imbalance\": 3.04,\n",
      "  \"left_x_avg_load\": 3.22,\n",
      "  \"left_y_avg_load\": 1.72,\n",
      "  \"left_z_avg_load\": 4.02,\n",
      "  \"left_x_contribution\": 36.0,\n",
      "  \"left_y_contribution\": 19.0,\n",
      "  \"left_z_contribution\": 45.0,\n",
      "  \"right_x_avg_load\": 3.42,\n",
      "  \"right_y_avg_load\": 2.77,\n",
      "  \"right_z_avg_load\": 3.81,\n",
      "  \"right_x_contribution\": 34.0,\n",
      "  \"right_y_contribution\": 28.0,\n",
      "  \"right_z_contribution\": 38.0,\n",
      "  \"v_imbalance\": 3.02,\n",
      "  \"ml_imbalance\": 23.38,\n",
      "  \"ap_imbalance\": 2.68\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "patient_metrics = compute_loading_metrics_from_assessment(left_data, right_data, filter_inputs, p)\n",
    "\n",
    "import json\n",
    "json_formatted_str = json.dumps(patient_metrics, indent=2)\n",
    "print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "48a5b47d-bd45-4160-94b9-6d24005ffcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "p_list = [\n",
    "    (1, \"665afc09824a6b1851b8c07f\", 0), \n",
    "    (2, \"665b03e0d7e141b790806601\", 0),\n",
    "    (3, \"665b03f4d7e141b790806602\", 0),\n",
    "    #(4, \"665b0403d7e141b790806603\", 1), no data in right sensor\n",
    "    (5, \"665b0411d7e141b790806604\", 1), \n",
    "    (6, \"665d7aa6f472136d4f0327e6\", 2), # under dir of walk pt6 - need to look at this\n",
    "    (7, \"665ece3fe8995c510afe35b4\", 2),\n",
    "    (8, \"6666a84bc5c65c1a0a814120\", 2),\n",
    "    (9, \"6666a8ebc5c65c1a0a814122\", 2),\n",
    "    (10,\"6666a864c5c65c1a0a814121\", 2),\n",
    "    (11,\"6666a96ac5c65c1a0a814123\", 3),\n",
    "    (12, \"666ffa646a8a304abfbbd386\", 3),\n",
    "    (13, \"667016b74f7e24d69c766b06\", 3),\n",
    "    (14, \"668d3cdcddb100ac4b670565\", 3),\n",
    "    (15, \"668d4591979e85e6b46658b6\", 3),\n",
    "    (16, \"669e187174ad33ebc71eb049\", 3),\n",
    "    (17, \"66b9e96d9f4026cf251ff723\", 1),\n",
    "    (18, \"66bb330314c767081f9c50a4\", 2),\n",
    "    (19, \"66bb4d4243348d3681fb0e8f\", 3),\n",
    "]\n",
    "\n",
    "# azure blob credentials - fetched on login as part of member api\n",
    "credentials = {\n",
    "    \"sas_token\": \"sp=racwdli&st=2024-01-28T10:36:12Z&se=2024-12-31T18:36:12Z&spr=https&sv=2022-11-02&sr=c&sig=xYPJ3bPvganjajOiUMm8wcfqjYfg1oI6dCFbryrS9hE%3D\",\n",
    "    \"storage_account_url\": \"https://rightstepdata.blob.core.windows.net\",\n",
    "    \"container_name\": \"rs-data\"\n",
    "}\n",
    "\n",
    "def get_patient_number(id, p_list):\n",
    "    p_num = 0\n",
    "    for num, id_str, grp in p_list:\n",
    "        if(id_str == id):\n",
    "            p_num = num\n",
    "    return p_num\n",
    "\n",
    "def blob_service_client(storage_account_url, sas_token):\n",
    "    return BlobServiceClient(account_url=storage_account_url, credential=sas_token)\n",
    "\n",
    "def blob_container_client(blob_service_client, container_name):\n",
    "    return blob_service_client.get_container_client(container=container_name)\n",
    "\n",
    "def get_files_from_azure(profile_id, container_client):\n",
    "    path = profile_id\n",
    "    files_to_download = container_client.list_blobs(path)\n",
    "    return files_to_download\n",
    "\n",
    "def make_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "def download_blobs(files, blob_service_client, container_name, patient_data_path):\n",
    "    local_file_list = []\n",
    "    for i, blob in enumerate(files):\n",
    "        blob_split = blob.name.split('/')\n",
    "        p_id = blob_split[1] #patient id unix\n",
    "        is_in_list = any(p_id == id_str for _, id_str, grp in p_list)\n",
    "        if(is_in_list):\n",
    "            patient_num = get_patient_number(p_id, p_list) ## patient number\n",
    "        \n",
    "            # make a dir for the patient unless it exisits\n",
    "            path = os.path.join(patient_data_path, str(patient_num))\n",
    "            make_directory(path)\n",
    "            index=0\n",
    "            # patient_num_timestamp_walk1_left.csv\n",
    "            file_name_split = blob_split[5].split(\"_\")\n",
    "            \n",
    "            if(\"Left\") in file_name_split:\n",
    "                index  = file_name_split.index(\"Left\")\n",
    "            elif(\"Right\") in file_name_split:\n",
    "                index  = file_name_split.index(\"Right\")\n",
    "            \n",
    "            local_file_name = str(patient_num) +\"_\" + file_name_split[2] + \"_\" + file_name_split[4] + \"_\" + file_name_split[index]+\".csv\"\n",
    "            local_dir = os.path.join(patient_data_path, str(patient_num) + \"/\")\n",
    "            local_path = local_dir + local_file_name\n",
    "            blob_client = blob_service_client.get_blob_client(container_name, blob, snapshot=None )\n",
    "            \n",
    "            with open(local_path, \"wb\") as my_blob:\n",
    "                blob_data = blob_client.download_blob()\n",
    "                blob_data.readinto(my_blob) # reads the data into the local file\n",
    "    return \"files downloaded\"\n",
    "    \n",
    "def get_patient_files(profile_id, creds, patient_data_path):\n",
    "    make_directory(patient_data_path)\n",
    "    sc = blob_service_client(creds[\"storage_account_url\"], creds[\"sas_token\"])\n",
    "    bcc = blob_container_client(sc, creds[\"container_name\"])\n",
    "    files = get_files_from_azure(profile_id, bcc)\n",
    "    response = download_blobs(files, sc, creds[\"container_name\"], patient_data_path)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2549675-bb47-4961-9a66-3ddb55866001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files downloaded\n"
     ]
    }
   ],
   "source": [
    "# download all the patients and compute the metrics for each\n",
    "profile_id = \"665afac88b3e08050604a8e6\"\n",
    "patient_data_path = \"patient_data/\"\n",
    "response = get_patient_files(profile_id, credentials, patient_data_path)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8bf91f0c-ce03-4594-a1b9-e9c62f925bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '5', '6', '7', '8', '9']\n",
      "['patient_data/6\\\\6_1717487224046_walk_Left.csv', 'patient_data/6\\\\6_1717487224046_walk_Right.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# List of directories to exclude\n",
    "excluded_dirs = {'.ipynb_checkpoints'}  # Use a set for faster lookups\n",
    "\n",
    "# List subdirectories, excluding the specified directories\n",
    "subdirs = [d for d in os.listdir(patient_data_path)\n",
    "           if os.path.isdir(os.path.join(patient_data_path, d)) and d not in excluded_dirs]\n",
    "print(subdirs)\n",
    "\n",
    "patient_data_files = []\n",
    "# Loop through each subdirectory\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(patient_data_path, subdir)    \n",
    "    # Use glob to find all files in the current subdirectory\n",
    "    files = glob.glob(os.path.join(subdir_path, '*.csv'))  # Get all files\n",
    "    if(subdir==str(6)):\n",
    "        print(files[len(files)-2 : len(files)])  # this is correct for patient 6\n",
    "    patient_data_files.append((subdir, files[len(files)-2 : len(files)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e960530-a535-463c-b5a3-a2e13e43ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute metrics for patient: 1\n",
      "compute metrics for patient: 10\n",
      "compute metrics for patient: 11\n",
      "compute metrics for patient: 12\n",
      "compute metrics for patient: 13\n",
      "compute metrics for patient: 14\n",
      "compute metrics for patient: 15\n",
      "compute metrics for patient: 16\n",
      "compute metrics for patient: 17\n",
      "compute metrics for patient: 18\n",
      "compute metrics for patient: 19\n",
      "compute metrics for patient: 2\n",
      "compute metrics for patient: 3\n",
      "compute metrics for patient: 5\n",
      "compute metrics for patient: 6\n",
      "compute metrics for patient: 7\n",
      "compute metrics for patient: 8\n",
      "compute metrics for patient: 9\n"
     ]
    }
   ],
   "source": [
    "patient_metrics = []\n",
    "for p_num, files in patient_data_files:\n",
    "    print(f\"compute metrics for patient: {p_num}\")\n",
    "    left_data = read_csv_to_dataframe(files[0])\n",
    "    right_data = read_csv_to_dataframe(files[1])\n",
    "    p_metrics = compute_loading_metrics_from_assessment(\n",
    "        left_data, \n",
    "        right_data, \n",
    "        filter_inputs,\n",
    "        p_num)\n",
    "    patient_metrics.append(p_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "323f37d7-1af2-4f62-a140-abb1a0678686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_patient_data = sorted(patient_metrics, key=lambda item: int(item['patient_number']), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61d61463-cbca-42e7-ae21-82766da7b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "p_df = pd.DataFrame(sorted_patient_data)\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = \"patient_metrics.csv\"\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "p_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb3f76-b407-4c8e-a85d-10610c1ca685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
